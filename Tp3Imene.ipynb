{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15028abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Téléchargement des ressources nécessaires de NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a229cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données à partir du fichier CSV\n",
    "df = pd.read_csv('movie_review.csv')\n",
    "\n",
    "# Définir les colonnes pour la classification\n",
    "features_column = 'text'  # Colonnes de caractéristiques\n",
    "labels_column = 'tag'     # Colonnes d'étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63035432",
   "metadata": {},
   "source": [
    "## Avant pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dad71b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        films adapted from comic books have had plenty...\n",
       "1        for starters , it was created by alan moore ( ...\n",
       "2        to say moore and campbell thoroughly researche...\n",
       "3        the book ( or \" graphic novel , \" if you will ...\n",
       "4        in other words , don't dismiss this film becau...\n",
       "                               ...                        \n",
       "64715    that lack of inspiration can be traced back to...\n",
       "64716    like too many of the skits on the current inca...\n",
       "64717    after watching one of the \" roxbury \" skits on...\n",
       "64718     bump unsuspecting women , and . . . that's all .\n",
       "64719    after watching _a_night_at_the_roxbury_ , you'...\n",
       "Name: text, Length: 64720, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc186c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données textuelles\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization des mots\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Conversion en minuscules\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Suppression de la ponctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Suppression des mots vides\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Reconstitution du texte prétraité\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Appliquer le prétraitement à chaque texte de la colonne \"Text\"\n",
    "df['Preprocessed_Text'] = df[features_column].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a39a2",
   "metadata": {},
   "source": [
    "## Apres pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360d659a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        films adapted comic books plenty success wheth...\n",
       "1        starters created alan moore eddie campbell bro...\n",
       "2        say moore campbell thoroughly researched subje...\n",
       "3        book `` graphic novel `` 500 pages long includ...\n",
       "4                            words n't dismiss film source\n",
       "                               ...                        \n",
       "64715      lack inspiration traced back insipid characters\n",
       "64716    like many skits current incarnation _saturday_...\n",
       "64717    watching one `` roxbury `` skits snl come away...\n",
       "64718                           bump unsuspecting women 's\n",
       "64719    watching _a_night_at_the_roxbury_ 'll left exa...\n",
       "Name: Preprocessed_Text, Length: 64720, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5480dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Word2Vec\n",
    "tokenized_texts = [word_tokenize(text) for text in df['Preprocessed_Text']]\n",
    "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943cf7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        films adapted comic books plenty success wheth...\n",
       "1        starters created alan moore eddie campbell bro...\n",
       "2        say moore campbell thoroughly researched subje...\n",
       "3        book `` graphic novel `` 500 pages long includ...\n",
       "4                            words n't dismiss film source\n",
       "                               ...                        \n",
       "64715      lack inspiration traced back insipid characters\n",
       "64716    like many skits current incarnation _saturday_...\n",
       "64717    watching one `` roxbury `` skits snl come away...\n",
       "64718                           bump unsuspecting women 's\n",
       "64719    watching _a_night_at_the_roxbury_ 'll left exa...\n",
       "Name: Preprocessed_Text, Length: 64720, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6690f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des reviews de movies\n",
    "def get_review_vector(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * 100  # Vecteur nul si aucun mot ne correspond dans le modèle\n",
    "\n",
    "df['Review_Vector'] = df['Preprocessed_Text'].apply(get_review_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46697067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-0.49658206, 0.31141835, 0.23791857, 0.056281...\n",
       "1        [-0.26951024, 0.30265602, 0.10224354, 0.188763...\n",
       "2        [-0.40534353, 0.37222615, 0.1986156, 0.0743688...\n",
       "3        [-0.6344722, 0.29854515, 0.21892644, 0.2255277...\n",
       "4        [-0.5824431, 0.6223749, -0.07175214, -0.206473...\n",
       "                               ...                        \n",
       "64715    [0.033346962, 0.39561346, 0.011281236, -0.2208...\n",
       "64716    [-0.4257373, 0.2883243, 0.22346158, 0.05913018...\n",
       "64717    [-0.85216767, 0.38365075, 0.4281632, 0.2064081...\n",
       "64718    [-0.33112565, 0.23932604, 0.33529422, 0.084547...\n",
       "64719    [-0.5578683, 0.2947772, 0.18137416, -0.2490791...\n",
       "Name: Review_Vector, Length: 64720, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review_Vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f01bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données\n",
    "X = pd.DataFrame(df['Review_Vector'].tolist())  # Caractéristiques\n",
    "y = df[labels_column]                           # Étiquettes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003ec6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction d'un classificateur (logistic regression)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97df17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5709981458590853\n",
      "Precision: 0.5718303233874755\n",
      "Recall: 0.5709981458590853\n",
      "F1 Score: 0.5675461155425728\n"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
