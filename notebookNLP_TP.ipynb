{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T19:47:40.567403Z","iopub.status.busy":"2024-02-24T19:47:40.567018Z","iopub.status.idle":"2024-02-24T19:48:42.452576Z","shell.execute_reply":"2024-02-24T19:48:42.451483Z","shell.execute_reply.started":"2024-02-24T19:47:40.567371Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n","[nltk_data]     failure in name resolution>\n","[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n","[nltk_data]     Temporary failure in name resolution>\n","[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n","[nltk_data]     failure in name resolution>\n","   tweet_id     author_id  inbound                      created_at  \\\n","0    119237        105834     True  Wed Oct 11 06:55:44 +0000 2017   \n","1    119238  ChaseSupport    False  Wed Oct 11 13:25:49 +0000 2017   \n","2    119239        105835     True  Wed Oct 11 13:00:09 +0000 2017   \n","3    119240  VirginTrains    False  Tue Oct 10 15:16:08 +0000 2017   \n","4    119241        105836     True  Tue Oct 10 15:17:21 +0000 2017   \n","\n","                                                text response_tweet_id  \\\n","0  @AppleSupport causing the reply to be disregar...            119236   \n","1  @105835 Your business means a lot to us. Pleas...               NaN   \n","2  @76328 I really hope you all change but I'm su...            119238   \n","3  @105836 LiveChat is online at the moment - htt...            119241   \n","4  @VirginTrains see attached error message. I've...            119243   \n","\n","   in_response_to_tweet_id  \n","0                      NaN  \n","1                 119239.0  \n","2                      NaN  \n","3                 119242.0  \n","4                 119240.0  \n"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","# Téléchargement des ressources NLTK\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Chargement des données\n","df = pd.read_csv(\"/kaggle/input/customer-support-on-twitter/sample.csv\")\n","print(df.head())\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T19:50:19.878593Z","iopub.status.busy":"2024-02-24T19:50:19.878218Z"},"trusted":true},"outputs":[],"source":["import nltk\n","nltk.download()\n","\n","# Fonctions de nettoyage\n","def preprocess_text(text):\n","    # Mise en minuscules\n","    text = text.lower()\n","    # Suppression des URL\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    # Suppression des balises HTML\n","    text = re.sub(r'<.*?>', '', text)\n","    # Suppression des émoticônes et des émojis\n","    text = re.sub(r'[^\\w\\s\\d]','', text)\n","    # Suppression des caractères spéciaux\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    return text\n","\n","# Appliquer la fonction de nettoyage au texte\n","df['text_cleaned'] = df['text'].apply(preprocess_text)\n","\n","# Supprimer les mots vides\n","stop_words = set(stopwords.words('english'))\n","df['text_cleaned'] = df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n","\n","# Instancier le Stemmer et le Lemmatizer\n","ps = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Fonctions de stemming et lemmatisation\n","def stem_text(text):\n","    return ' '.join([ps.stem(word) for word in text.split()])\n","\n","def lemmatize_text(text):\n","    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n","\n","# Appliquer stemming et lemmatisation\n","df['text_cleaned'] = df['text_cleaned'].apply(stem_text)\n","df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_text)\n","\n","# Afficher le DataFrame avec les colonnes nécessaires\n","df = df[['tweet_id', 'text', 'text_cleaned']]\n","\n","# Enregistrer le DataFrame modifié\n","df.to_csv(\"cleaned_dataset.csv\", index=False)\n","\n","# Afficher les premières lignes du DataFrame\n","print(df.head())"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4133,"sourceId":8841,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
